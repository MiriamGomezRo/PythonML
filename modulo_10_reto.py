# -*- coding: utf-8 -*-
"""Modulo 10 Reto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13zKGhI2COweRi3GVa8Qf75xcroZFRmWq

# Modulo 10 - Reto
"""

#Bibliotecas para poder trabajar con Spark

!sudo apt update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz
!tar xf spark-3.2.4-bin-hadoop3.2.tgz
#Configuración de Spark con Python
!pip install -q findspark
!pip install pyspark

#Estableciendo variable de entorno
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.4-bin-hadoop3.2"

#Buscando e inicializando la instalación de Spark
import findspark
findspark.init()
findspark.find()

from pyspark.sql import SparkSession

df_ml = SparkSession.builder.appName('EjemploML').getOrCreate()
df_ml

#Cargamos la información en un DataFrame
training_dataset  = df_ml.read.csv('Monterrey Pollution Data 2.csv', header=True, inferSchema=True)
training_dataset

"""Renombrar "PM2.5" para mejor analisis"""

training_dataset = training_dataset.withColumnRenamed("PM2.5", "PM25")

#Mostramos la información
training_dataset.show()

#Visualizamos el esquema de la base de datos
training_dataset.printSchema()

#Visualizamos el nombre de las columnas
training_dataset.columns

"""# **Analisis de O3**

**Cargar datos y eliminar variables innecesarias( date, Day y DayWeek ), creamos el vectorAssembler sin variables output O3 ni PM2.5 y sin las variables Date y DayWeek  **

**Independent Features**
"""

from pyspark.ml.feature import VectorAssembler

featassembler = VectorAssembler(inputCols=
                                ['Month',
                                'WeekDay',
                                'Hour',
                                'CO',
                                'NO',
                                'NO2',
                                'NOX',
                                'PM10',
                                'PRS',
                                'RAINF',
                                'RH',
                                'SR',
                                'TOUT',
                                'WSR',
                                'WDV']
                                , outputCol = "Independent Features" )
featassembler

result = featassembler.transform(training_dataset)
result.show()

"""Features Extended, incluir variable O3"""

from pyspark.ml.feature import VectorAssembler

featassembler = VectorAssembler(inputCols=
                                ['O3',
                                'Month',
                                'WeekDay',
                                'Hour',
                                'CO',
                                'NO',
                                'NO2',
                                'NOX',
                                'PM10',
                                'PRS',
                                'RAINF',
                                'RH',
                                'SR',
                                'TOUT',
                                'WSR',
                                'WDV'], outputCol = "Features Extended" )
featassembler

result2 = featassembler.transform(training_dataset)
result2.show()

"""Para construir nuestro modelo de regresión debemos selecccionar la columna que integró los valores de las columnas en un vector y la columna que representa la variable dependiente."""

final_data = result.select("Independent features", "O3")
final_data.show()

"""Del conjunto total de datos se puede generar un división de conjunto de datos entre entrenamiento y prueba con la función randomSplit."""

train_data, test_data = final_data.randomSplit([0.75, 0.25])

"""Aplicar Linear Regression en PySpark"""

from pyspark.ml.regression import LinearRegression

model = LinearRegression(featuresCol = 'Independent features', labelCol='O3')
model = model.fit(train_data)

"""Podemos imprimir la matriz de correlación para verificar la congruencia del modelo."""

from pyspark.ml.stat import Correlation

matrix = Correlation.corr(final_data, 'Independent features')
cor_np = matrix.collect()[0][matrix.columns[0]].toArray()
cor_np

#Analisis de multicolinealidad- sin variable dependiente

inputCols=                     ['Month', #1

                                'WeekDay',#3
                                'Hour',  #4
                                'CO',    #5
                                'NO',    #6
                                'NO2',   #7
                                'NOX',   #8
                                'PM10',  #9
                                'PRS',  #10
                                'RAINF',#11
                                'RH',   #12
                                'SR',   #13
                                'TOUT', #14
                                'WSR',  #15
                                'WDV']  #16
  # Relacion Month - CO:  -0.54861099
  # Relacion Month - NO2: -0.59194504
  # Relacion Month - TOUT: 0.67349906
  # Relacion CO-Month :-0.54861099
  # Relacion CO - NO: 0.5978396
  # Relacion CO - NO2: 0.67806613
  # Relacion CO - NOX : 0.69620186
  # Relacion NO - CO : 5978396
  # Relacion NO - NOX: 95203112
  # Relacion NO2- Month: -0.59194504
  # Relacion NO2- CO: 0.67806613
  # Relacion NO2 -NOX : 0.78844922
  # Relacion NO2- PM10: 0.55036066
  # Relacion NOX -CO :0.69620186
  # Relacion NOX -NO :0.95203112
  # Relacion NOX- NO2: 0.78844922
  # Relacion NOX- PM10: 0.56135302
  # Relacion PM10- NO2 : 0.55036066
  # Relacion PM10 -NOX : - 0.56135302
  # Relacion RH - TOUT: -0.55091045
  # Relacion RH - PRS : -0.66116281
  # Relacion RH - TOUT : -0.55091045

matrix = Correlation.corr(result2, 'Features Extended')
cor_np = matrix.collect()[0][matrix.columns[0]].toArray()
cor_np

# Analisis de congruencia con variable dependiente
# Checar el primer renglon de la matriz y verificar coeficientes que sean iguales , los que son diferentes se eliminan

inputCols=                      ['O3',#- Variable Output
                                'Month', # -- +
                                'WeekDay',#-- +
                                'Hour',  # -- +
                                'CO',  # -- "-"
                                'NO',# -- "-"
                                'NO2', # -- "-"
                                'NOX',# -- " - "
                                'PM10',# --" - "
                                'PRS',# -- "- "
                                'RAINF',# -- " - "
                                'RH',# -- "-"
                                'SR', #-- +
                                'TOUT', # -- +
                                'WSR', # -- +
                                'WDV'] # -- "-"

model.intercept #B0

model.coefficients

#Variables que cambiaron de signo :
#'Month', # -- + -->+
#'Day',   #-- + --> - CAMBIO
#'WeekDay',#-- + --> +
#'Hour',  # -- + --> - CAMBIO
#'CO',  # -- "-"--> + CAMBIO
#'NO',# -- "-"--> -
#'NO2', # -- "-" --> -
#'NOX',# -- " - "-->+ CAMBIO
#'PM10',# --" - "-->+ CAMBIO
#'PRS',# -- "- "--> -
#'RAINF',# -- " - "--> + CAMBIO
#'RH',# -- "-"--> -
#'SR', #-- + --> +
#'TOUT', # -- + --> - CAMBIO
#'WSR', # -- + --> +
#'WDV' # -- "-"-->-

#Realizar el analisis sin estas variables :  Day, Hour, CO, NOX, PM10, RAINF y TOUT

model.summary.pValues

model.summary.r2adj

prediction_result = model.evaluate(test_data)
prediction_result.predictions.show()

"""Indicadores de Desempeño"""

prediction_result.meanAbsoluteError, prediction_result.meanSquaredError

"""# **Realizar el analisis sin estas variables :  Day, Hour, CO, NOX, PM10, RAINF y TOUT** (O3)

Tomar en cuenta que el PValues de la variable PM10 es de  6.106226635438361e-14, y ya se esta eliminando en el cambio de signo por el analisis de congruencia

Considerar Quitar las Variables  **NO y NO2** por que tienen alta correlacion e indica multicolinealidad

Independent Features
"""

from pyspark.ml.feature import VectorAssembler

featassembler = VectorAssembler(inputCols=
                                ['Month',
                                'WeekDay',
                                #'NO',
                                #'NO2',
                                'PRS',
                                'RH',
                                'SR',
                                'WSR',
                                'WDV']
                                , outputCol = "Independent Features" )
featassembler

result = featassembler.transform(training_dataset)
result.show()

"""**Extended Features incluir variable O3**"""

from pyspark.ml.feature import VectorAssembler

featassembler = VectorAssembler(inputCols=
                                ['O3',
                                'Month',
                                'WeekDay',
                                #'NO',
                                #'NO2',
                                'PRS',
                                'RH',
                                'SR',
                                'WSR',
                                'WDV'], outputCol = "Features Extended" )

result2 = featassembler.transform(training_dataset)
result2.show()

final_data = result.select("Independent features", "O3")
final_data.show()

"""Partir en Train y test dataframes"""

train_data, test_data = final_data.randomSplit([0.75, 0.25])

"""Aplicar Lineal Regression"""

from pyspark.ml.regression import LinearRegression

model = LinearRegression(featuresCol = 'Independent features', labelCol='O3')
model = model.fit(train_data)

"""Podemos imprimir la matriz de correlación para verificar la congruencia del modelo."""

matrix = Correlation.corr(final_data, 'Independent features')
cor_np = matrix.collect()[0][matrix.columns[0]].toArray()
cor_np

"""Analisis de congrencia"""

matrix = Correlation.corr(result2, 'Features Extended')
cor_np = matrix.collect()[0][matrix.columns[0]].toArray()
cor_np

"""Obtener el valor del intercepto."""

model.intercept

"""Los coeficientes por cada variable independiente"""

model.coefficients

"""Así como los p-values para determinar la transendencia de cada variable dentro del modelo."""

model.summary.pValues

model.summary.r2adj

"""Podemos realizar predicciones para evaluar el modelo obtenido."""

prediction_result = model.evaluate(test_data)
prediction_result.predictions.show()

"""Mostrar algunos indicadores de desempeño utiles."""

prediction_result.meanAbsoluteError, prediction_result.meanSquaredError

"""# **Analizar la Variable PM2.5**

Crear Independent Features sin la Variable PM2.5
"""

from pyspark.ml.feature import VectorAssembler

featassembler = VectorAssembler(inputCols=
                                ['Month',

                                'WeekDay',
                                'Hour',
                                'CO',
                                'NO',
                                'NO2',
                                'NOX',
                                'PM10',
                                'PRS',
                                'RAINF',
                                'RH',
                                'SR',
                                'TOUT',
                                'WSR',
                                'WDV']
                                , outputCol = "Independent Features" )
featassembler

result = featassembler.transform(training_dataset)
result.show()

featassembler = VectorAssembler(inputCols=
                                ['PM25',
                                'Month',
                                'WeekDay',
                                'Hour',
                                'CO',
                                'NO',
                                'NO2',
                                'NOX',
                                'PM10',
                                'PRS',
                                'RAINF',
                                'RH',
                                'SR',
                                'TOUT',
                                'WSR',
                                'WDV'], outputCol = "Features Extended" )
featassembler

result2 = featassembler.transform(training_dataset)
result2.show()

"""Para construir nuestro modelo de regresión debemos selecccionar la columna que integró los valores de las columnas en un vector y la columna que representa la variable dependiente."""

final_data = result.select("Independent features", "PM25")
final_data.show()

"""Partir el DF en Train y Test"""

train_data, test_data = final_data.randomSplit([0.75, 0.25])

"""Ahora bien hay que importar LinearRegression de la biblioteca de machine learning de PySpark. Especificando cuales son la variables independientes y cual es la dependiente."""

from pyspark.ml.regression import LinearRegression

model = LinearRegression(featuresCol = 'Independent features', labelCol='PM25')
model = model.fit(train_data)

from pyspark.ml.stat import Correlation

matrix = Correlation.corr(final_data, 'Independent features')
cor_np = matrix.collect()[0][matrix.columns[0]].toArray()
cor_np

# Analisis de multicolinealidad Independent Features
inputCols=['Month',
                                'WeekDay',
                                'Hour',
                                'CO',
                                'NO',
                                'NO2',
                                'NOX',
                                'PM10',
                                'PRS',
                                'RAINF',
                                'RH',
                                'SR',
                                'TOUT',
                                'WSR',
                                'WDV']
  # Relacion Month - CO:  -0.54861099
  # Relacion Month - NO2: -0.59194504
  # Relacion Month - TOUT: 0.67349906
  # Relacion CO-Month :-0.54861099
  # Relacion CO - NO: 0.5978396
  # Relacion CO - NO2: 0.67806613
  # Relacion CO - NOX : 0.69620186
  # Relacion NO - CO : 5978396
  # Relacion NO - NOX: 95203112
  # Relacion NO2- Month: -0.59194504
  # Relacion NO2- CO: 0.67806613
  # Relacion NO2 -NOX : 0.78844922
  # Relacion NO2- PM10: 0.55036066
  # Relacion NOX -CO :0.69620186
  # Relacion NOX -NO :0.95203112
  # Relacion NOX- NO2: 0.78844922
  # Relacion NOX- PM10: 0.56135302
  # Relacion PM10- NO2 : 0.55036066
  # Relacion PM10 -NOX : - 0.56135302
  # Relacion RH - TOUT: -0.55091045
  # Relacion RH - PRS : -0.66116281
  # Relacion RH - TOUT : -0.55091045

matrix = Correlation.corr(result2, 'Features Extended')
cor_np = matrix.collect()[0][matrix.columns[0]].toArray()
cor_np

#Analisis de congruencia , incluir la variable output PM25 (antes PM2.5)
inputCols=['PM25',
                                'Month',# - --> +  CAMBIO
                                'WeekDay',# + --> +
                                'Hour',# + --> +
                                'CO', # + --> +
                                'NO',# + --> +
                                'NO2',# + --> +
                                'NOX',# + --> -  CAMBIO
                                'PM10',# + --> +
                                'PRS',# - --> -
                                'RAINF',#+ --> +
                                'RH',#+ --> +
                                'SR',#- -- > -
                                'TOUT',#+ --> +
                                'WSR',#- --> +  CAMBIO
                                'WDV'] # - --> -

"""Obtener el valor del intercepto."""

model.intercept

"""Los coeficientes por cada variable independiente"""

model.coefficients

"""Así como los p-values para determinar la transendencia de cada variable dentro del modelo."""

model.summary.pValues

"""Obtener indicadores de desempeño como la  r2  ajustada, dado que es un problema multivariado. Que nos sirve para indicar el porcentaje de la variabilidad de la variable dependiente explicada por el modelo."""

model.summary.r2adj

prediction_result = model.evaluate(test_data)
prediction_result.predictions.show()

"""Mostrar algunos indicadores de desempeño utiles."""

prediction_result.meanAbsoluteError, prediction_result.meanSquaredError

"""Conclusion : Desarrollar un Analisis sin las variables que cambiaron de signo en el analisis de congruencia : Month, NOX y WSR,  tambien quitar la variable PM10 ya que su valor de P es  2.081045336055354e-09,

Independent Features sin variables Month, NOX y WSR

Considerar quitar las Variables NO y NO2 por que tienen alta correlacion e indica multicolinealidad
"""

from pyspark.ml.feature import VectorAssembler

featassembler = VectorAssembler(inputCols= ['Day',
                                'WeekDay',
                                'Hour',
                                'CO',
                               # 'NO',
                               # 'NO2',
                                'PRS',
                                'RAINF',
                                'RH',
                                'SR',
                                'TOUT',
                                'WDV']
                                , outputCol = "Independent Features" )
featassembler

result = featassembler.transform(training_dataset)
result.show()

"""Construir **Features Extended** con la variable PM25(Output) y sin las variables (Month, NOX y WSR),"""

featassembler = VectorAssembler(inputCols=
                                ['PM25',
                                'WeekDay',
                                'Hour',
                                'CO',
                                #'NO',
                               # 'NO2',
                                'PRS',
                                'RAINF',
                                'RH',
                                'SR',
                                'TOUT',
                                'WDV'], outputCol = "Features Extended" )
featassembler

"""Posteriormente se integran al conjunto de datos que ya estaba cargado utilizando la función transform()."""

result2 = featassembler.transform(training_dataset)
result2.show()

"""Para construir nuestro modelo de regresión debemos selecccionar la columna que integró los valores de las columnas en un vector y la columna que representa la variable dependiente."""

final_data = result.select("Independent features", "PM25")
final_data.show()

"""Del conjunto total de datos se puede generar un división de conjunto de datos entre entrenamiento y prueba con la función randomSplit."""

train_data, test_data = final_data.randomSplit([0.75, 0.25])

from pyspark.ml.regression import LinearRegression

model = LinearRegression(featuresCol = 'Independent features', labelCol='PM25')
model = model.fit(train_data)

"""Podemos imprimir la matriz de correlación para verificar la congruencia del modelo."""

from pyspark.ml.stat import Correlation

matrix = Correlation.corr(final_data, 'Independent features')
cor_np = matrix.collect()[0][matrix.columns[0]].toArray()
cor_np

matrix = Correlation.corr(result2, 'Features Extended')
cor_np = matrix.collect()[0][matrix.columns[0]].toArray()
cor_np

"""Obtener el valor del intercepto."""

model.intercept
#B0

"""Los coeficientes por cada variable independiente"""

model.coefficients

model.summary.pValues

model.summary.r2adj

"""Podemos realizar predicciones para evaluar el modelo obtenido."""

prediction_result = model.evaluate(test_data)
prediction_result.predictions.show()

prediction_result.meanAbsoluteError, prediction_result.meanSquaredError