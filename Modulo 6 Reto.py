# -*- coding: utf-8 -*-
"""Modulo6Reto - Copy (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IZp3msWkuhLr_sLq2kzGCvNhTfPsuv75

# Modulo 6 RETO- Employees Dataset

**Tema1: Familiarizacion y limpieza de datos**
"""

#Paqueteria
import pandas as pd

# Conexión con google drive
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/Colab Notebooks/Modulo 6/2023/Reto"

# Lectura de Datos pd.read_csv("Employees.csv")
datos = pd.read_csv("Employees.csv")
datos.head()

#Dimension de los datos
datos.shape

"""**Tema 2: Tratamiento de variables e ingenieria de caracteristicas**"""

#Valores Perdidos
pd.isna(datos).sum()

#saber tipos de datos
datos.info()

#Imputar campo de AGE , Time_of_service, Work_Life_balance y pay_scale
#Median Age
datos['Age'].fillna(datos['Age'].median(), inplace=True)
datos['Time_of_service'].fillna(datos['Time_of_service'].median(), inplace=True)
datos['Work_Life_balance'].fillna(datos['Work_Life_balance'].median(), inplace=True)
datos['Pay_Scale'].fillna(datos['Pay_Scale'].median(), inplace=True)
#Checar valores perdidos otra vez
pd.isna(datos).sum()

"""**Split Features** Cambiar de lugar"""

# tratar de conocer el contendo de la columna UNIT
datos['Unit'].unique()

units = datos['Unit'].str.split(" ", n =2, expand=True)
units.columns=['Unit_0', 'Unit_1', 'Unit_2']
units

#unit con datos
datos = datos.join(units)
#borrar columna unit
datos=datos.drop(['Unit'], axis = 1)

"""# **One Hot Encoding ** cambiar de lugar"""

#Variable binaria Gender
datos_gender_dummies=pd.get_dummies(datos,columns=['Gender'])
datos_gender_dummies = datos_gender_dummies.drop(['Gender_M'], axis=1)
datos_gender_dummies

#Variable Binaria Relationship_Status
datos_Relationship_Status_dummies=pd.get_dummies(datos_gender_dummies,columns=['Relationship_Status'])
datos_Relationship_Status_dummies = datos_Relationship_Status_dummies.drop(['Relationship_Status_Single'], axis=1)
datos_Relationship_Status_dummies

#Variable Categorica Hometown
datos_Hometown_dummies=pd.get_dummies(datos_Relationship_Status_dummies,columns=['Hometown'])
datos_Hometown_dummies

#Variable Categorica Decision_skill_possess
datos_Decision_skill_possess_dummies=pd.get_dummies(datos_Hometown_dummies,columns=['Decision_skill_possess'])
datos_Decision_skill_possess_dummies

#Variable Categorica Compensation_and_Benefits
datosDummies=pd.get_dummies(datos_Decision_skill_possess_dummies,columns=['Compensation_and_Benefits'])
datosDummies

datosDummies.shape

datosDummies.describe()

datosDummies.info()

"""# **Analizar variables numericas : **

Age,Education_Level,Time_of_service,Time_since_promotion,growth_rate,Travel_Rate,Post_Level,Pay_Scale,Work_Life_balance
"""

#Variable AGE
boxplotAge = datosDummies.boxplot(column=['Age'])
boxplotAge

#Variable Education_Level
boxplotEducation_Level = datosDummies.boxplot(column=['Education_Level'])
boxplotEducation_Level

# Proceso estadístico con la desviación estándar para quitar valores atipicos

factor = 2 #probar con 2 y 3
upper_lim = datosDummies['Education_Level'].mean() + datosDummies['Education_Level'].std()*factor
print(upper_lim)
lower_lim = datosDummies['Education_Level'].mean() - datosDummies['Education_Level'].std()*factor
print(lower_lim)

# Valores que están fuera de estos límites

datosDummies = datosDummies[(datosDummies['Education_Level'] < upper_lim) & (datosDummies['Education_Level'] > lower_lim)]
datosDummies.shape

boxplotEducation_Level = datosDummies.boxplot(column=['Education_Level'])
boxplotEducation_Level

#Variable Time_of_service
boxplotTime_of_service = datosDummies.boxplot(column=['Time_of_service'])
boxplotTime_of_service

# Proceso estadístico con la desviación estándar para quitar valores atipicos

factor = 2 #probar con 2 y 3
upper_lim = datosDummies['Time_of_service'].mean() + datosDummies['Time_of_service'].std()*factor
print(upper_lim)
lower_lim = datosDummies['Time_of_service'].mean() - datosDummies['Time_of_service'].std()*factor
print(lower_lim)

# Valores que están fuera de estos límites

datosDummies = datosDummies[(datosDummies['Time_of_service'] < upper_lim) & (datosDummies['Time_of_service'] > lower_lim)]
datosDummies.shape

datosDummies.describe()

#Variable Time_of_service despues de seleccion con proceso estadistico
boxplotTime_of_service = datosDummies.boxplot(column=['Time_of_service'])
boxplotTime_of_service

#Variable Time_since_promotion
boxplotTime_since_promotion = datosDummies.boxplot(column=['Time_since_promotion'])
boxplotTime_since_promotion

#Variable growth_rate
boxplotgrowth_rate = datosDummies.boxplot(column=['growth_rate'])
boxplotgrowth_rate

#Variable Travel_Rate
boxplotTravel_Rate = datosDummies.boxplot(column=['Travel_Rate'])
boxplotTravel_Rate

#Variable Post_Level
boxplotPost_Level = datosDummies.boxplot(column=['Post_Level'])
boxplotPost_Level

# Proceso estadístico con la desviación estándar para quitar valores atipicos

factor = 2 #probar con 2 y 3
upper_lim = datosDummies['Post_Level'].mean() + datosDummies['Post_Level'].std()*factor
print(upper_lim)
lower_lim = datosDummies['Post_Level'].mean() - datosDummies['Post_Level'].std()*factor
print(lower_lim)

# Valores que están fuera de estos límites

datosDummies = datosDummies[(datosDummies['Post_Level'] < upper_lim) & (datosDummies['Post_Level'] > lower_lim)]
datosDummies.shape

boxplotPost_Level = datosDummies.boxplot(column=['Post_Level'])
boxplotPost_Level

#Variable Pay_Scale
boxplotPay_Scale = datosDummies.boxplot(column=['Pay_Scale'])
boxplotPay_Scale

#Variable Work_Life_balance
boxplotWork_Life_balance = datosDummies.boxplot(column=['Work_Life_balance'])
boxplotWork_Life_balance

"""#Transformación logística (Y), Crear histogramas por cada variable numerica para verificar el sesgo"""

import numpy as np
import matplotlib.pyplot as plt

"""**AGE**"""

datosDummies['Age'].skew()
# sesgo a la derecha y no pareciera una distribución normal

import matplotlib.pyplot as plt
plt.hist(datosDummies.Age, bins = 500)

datosDummies['Age_log']= (datosDummies['Age']+1).transform(np.log)
datosDummies

plt.hist(datosDummies.Age_log, bins = 500)

"""**Education_Level**"""

datosDummies['Education_Level'].skew()
# sesgo a la izquierda y no pareciera una distribución normal

plt.hist(datosDummies.Education_Level, bins = 500)

datosDummies['Education_Level_log']= (datosDummies['Education_Level']+1).transform(np.log)
datosDummies

plt.hist(datosDummies.Education_Level_log, bins = 500)

"""**Time_of_service**"""

datosDummies['Time_of_service'].skew()
# sesgo a la derecha y no pareciera una distribución normal

plt.hist(datosDummies.Time_of_service, bins = 20)

datosDummies['Time_of_service_log']= (datosDummies['Time_of_service']+1).transform(np.log)
plt.hist(datosDummies.Time_of_service_log, bins = 10)

"""**Time_since_promotion**"""

datosDummies['Time_since_promotion'].skew()
# sesgo a la derecha y no pareciera una distribución normal

plt.hist(datosDummies.Time_since_promotion, bins = 5)

datosDummies['Time_since_promotion_log']= (datosDummies['Time_since_promotion']+1).transform(np.log)
plt.hist(datosDummies.Time_since_promotion_log, bins = 5)

"""**growth_rate**"""

datosDummies['growth_rate'].skew()
# sesgo a la izquierda y no pareciera una distribución normal

plt.hist(datosDummies.growth_rate, bins = 20)

datosDummies['growth_rate_log']= (datosDummies['growth_rate']+1).transform(np.log)

plt.hist(datosDummies.growth_rate_log, bins = 20)

"""**Travel_Rate**"""

datosDummies['Travel_Rate'].skew()
# sesgo a la derecha con una forma normal

plt.hist(datosDummies.Travel_Rate, bins = 3)

"""**Post_Level**"""

datosDummies['Post_Level'].skew()
# sesgo a la derecha

plt.hist(datosDummies.Post_Level, bins = 5)

"""**Pay_Scale**"""

datosDummies['Pay_Scale'].skew()
# sesgo a la izquierda , tiene grafica normal

plt.hist(datosDummies.Pay_Scale, bins = 5)

"""**Work_Life_balance**"""

datosDummies['Work_Life_balance'].skew()
# sesgo a la derecha y no pareciera una distribución normal

plt.hist(datosDummies.Work_Life_balance, bins = 10)

datosDummies['Work_Life_balance_log']= (datosDummies['Work_Life_balance']+1).transform(np.log)

plt.hist(datosDummies.Work_Life_balance_log, bins = 10)

datosDummies.describe()

from google.colab import drive
drive.mount('/content/drive')

datosDummies.to_csv('datosDummies.csv')
#!cp datosDummies.csv "drive/MyDrive/Colab Notebooks/Modulo 6/2023/Reto"

### Estandarización: Este concepto se refiere a hacer que la distribución de datos sea normal. Transforma la media de los datos en 0 y su varianza en 1. ##MÁS COMÚN
columns = ['Employee_ID','Attrition_rate','Unit_0','Unit_1', 'Unit_2', 'Gender_F', 'Relationship_Status_Married','Hometown_Clinton', 'Hometown_Franklin','Hometown_Lebanon','Hometown_Springfield', 'Hometown_Washington', 'Decision_skill_possess_Analytical', 'Decision_skill_possess_Behavioral','Decision_skill_possess_Conceptual', 'Decision_skill_possess_Directive', 'Compensation_and_Benefits_type0', 'Compensation_and_Benefits_type1', 'Compensation_and_Benefits_type2', 'Compensation_and_Benefits_type3', 'Compensation_and_Benefits_type4']
categoricas = pd.DataFrame(datosDummies, columns=columns) #variables categoricas y variable respuesta (y)
categoricas

numericas = datosDummies.drop(['Employee_ID','Attrition_rate','Unit_0','Unit_1', 'Unit_2', 'Gender_F', 'Relationship_Status_Married','Hometown_Clinton', 'Hometown_Franklin','Hometown_Lebanon','Hometown_Springfield', 'Hometown_Washington', 'Decision_skill_possess_Analytical', 'Decision_skill_possess_Behavioral','Decision_skill_possess_Conceptual', 'Decision_skill_possess_Directive', 'Compensation_and_Benefits_type0', 'Compensation_and_Benefits_type1', 'Compensation_and_Benefits_type2', 'Compensation_and_Benefits_type3', 'Compensation_and_Benefits_type4'], axis = 1)
numericas

"""# Escalar las variables numericas sin Attrition RATE"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled = scaler.fit_transform(numericas)
scaled

numericas = pd.DataFrame(scaled)
numericas.columns= ['Age', 'Education_Level', 'Time_of_service', 'Time_since_promotion', 'growth_rate', 'Travel_Rate', 'Post_Level', 'Pay_Scale', 'Work_Life_balance', 'Age_log', 'Education_Level_log', 'Time_of_service_log', 'Time_since_promotion_log', 'growth_rate_log', 'Work_Life_balance_log']
numericas

numericas = numericas.round (2)
numericas
numericasFA = numericas

numericas.info()

categoricas.info()

"""**Tema 3: Reduccion de dimensionalidad**

# **PCA**
"""

# Tratamiento de datos
# ==============================================================================
import numpy as np
import pandas as pd
import statsmodels.api as sm

# Preprocesado y modelado
# ==============================================================================
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import scale

# Configuración warnings
# ==============================================================================
import warnings
warnings.filterwarnings('ignore')

pca_model = PCA(n_components = 15)
pca_model.fit(numericas)

pca_model.components_

# Se combierte el array a dataframe para añadir nombres a los ejes. #vector de loadings, cargas

pd.DataFrame(
    data    = pca_model.components_,
    columns = numericas.columns,
    index   = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10','PC11', 'PC12', 'PC13', 'PC14', 'PC15']
)

# Porcentaje de varianza explicada acumulada ¿Con cuántos componentes se quedarían?
# ==============================================================================
prop_varianza_acum = pca_model.explained_variance_ratio_.cumsum()
print('------------------------------------------')
print('Porcentaje de varianza explicada acumulada')
print('------------------------------------------')
print(prop_varianza_acum)

proyecciones = np.dot(pca_model.components_, scale(numericas).T)
proyecciones = pd.DataFrame(proyecciones, index = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10','PC11', 'PC12', 'PC13', 'PC14', 'PC15' ])
proyecciones = proyecciones.transpose().set_index(numericas.index)
proyecciones

"""Explicar el 100 % de las relaciones"""

categoricas[['Attrition_rate']].info()

proyecciones.info()

proyecciones2 = proyecciones
conjunto_1_Attrition_rate = proyecciones.join(categoricas[['Attrition_rate']])
#conjunto_1_Attrition_rate= pd.concat([proyecciones, categoricas[['Attrition_rate']]], axis = 1)
conjunto_1_Attrition_rate

"""**Exportar a csv solo Attrition_Rate**"""

from google.colab import drive
drive.mount('/content/drive')
conjunto_1_Attrition_rate.to_csv('conjunto_1_Attrition_rate.csv')

"""**Exportar resto de variables categoricas**"""

conjunto_1_Categoricas = proyecciones2.join(categoricas)
#conjunto_1_Categoricas= pd.concat([proyecciones, categoricas], axis = 1)
conjunto_1_Categoricas

from google.colab import drive
drive.mount('/content/drive')
conjunto_1_Categoricas.to_csv('conjunto_1_Categoricas.csv')

"""# **Analisis Factorial **"""

!pip install factor_analyzer
# Tratamiento de datos
# ==============================================================================
import numpy as np
import pandas as pd
import statsmodels.api as sm

# Preprocesado y modelado
# ==============================================================================
from factor_analyzer import FactorAnalyzer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import scale


# Configuración warnings
# ==============================================================================
import warnings
warnings.filterwarnings('ignore')

numericasFA.head()

print('Media de cada variable')
numericasFA.mean()

print('Desviación estándar de cada variable')
datos.std()

#Prueba de Barlett para saber si utilizar FA es buena opción

from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
chi_square_value,p_value=calculate_bartlett_sphericity(numericasFA)
chi_square_value, p_value

# Crear un análisis factorial sin especificaciones de factores para poder extraer
# los valores de los eigenvalores y determinar el número de factores a utilizar
fa = FactorAnalyzer()
fa.fit(numericasFA)
# Check Eigenvalues
ev, v= fa.get_eigenvalues()
ev

"""solo se pueden tener 8 factores por que los eivenvalues los primeros son mayores que 1 o cercanos
:
 3.85564743, 2.04190256, 1.97811137, 1.94671812, 1.83344606,1.0239143 , 1.01040746, 0.96190351,
"""

fa = FactorAnalyzer(n_factors = 8) #1 o 2
fa.fit(numericasFA)

pd.DataFrame(fa.loadings_)

# Se combierte el array a dataframe para añadir nombres a los ejes. #vector de loadings, cargas
pd.DataFrame(
    data    =fa.loadings_,
    columns = ['Factor1','Factor2', 'Factor3', 'Factor4', 'Factor5', 'Factor6', 'Factor7', 'Factor8'],
    index   = numericasFA.columns
)

# Obtener la varianza
fa.get_factor_variance()

# transformar con el Fa con 8 factores
datos_fa= pd.DataFrame(fa.transform(numericasFA))
datos_fa

datos_fa2 = datos_fa
conjunto_fa_Attrition_rate=datos_fa.join(categoricas[['Attrition_rate']])
#conjunto_fa_Attrition_rate = pd.concat([datos_fa, categoricas[['Attrition_rate']]], axis = 1)
conjunto_fa_Attrition_rate

"""# **Guardar INFO **"""

from google.colab import drive
drive.mount('/content/drive')
conjunto_fa_Attrition_rate.to_csv('conjunto_fa_Attrition_rate.csv')

#conjunto_fa_Categoricas = pd.concat([datos_fa, categoricas], axis = 1)
conjunto_fa_Categoricas = datos_fa2.join(categoricas)
conjunto_fa_Categoricas

from google.colab import drive
drive.mount('/content/drive')
conjunto_fa_Categoricas.to_csv('conjunto_fa_Categoricas.csv')

"""**Conclusiones **

*   Se detectaron valores faltantes en las siguientes variables : Age, Time_of_service, Work_Life_balance y Pay_Scale las cuales se llenaron con la median ya que son valores numéricos.
*   En variable UNIT se tubo que hacer SPLIT FEATURES de 3 columnas por que se apreciaban valores de hasta 3 posiciones
*   Se tuvo que hacer ONE HOT ENCODIG (GET DUMMIES) a las siguientes variables Categoricas : Gender, Relationship_Status, Hometown, Decision_skill_possess, Compensation_and_Benefits, las primeras 2 variables son categóricas binarias
*   Se realizaron boxplot de las variables numéricas para observar valores atípicos
*   De las variables observadas se trato de realizar la  selección de los renglones : Education_Level,Time_of_service, y Post_Level ya que se observaron outliers en los diagramas de boxplot
*   De las siguientes variables se realizo transformación logarítmica por que se observo un sesgo hacia la derecha o hacia la izquierda por medio de la instruccion .skew(): AGE, Education_Level, Time_of_service, Time_since_promotion, growth_rate, Work_Life_balance,
*   Se separaron los datos en variables categóricas y numéricas
*   Al aplicar PCA en el dataframe de variables numéricas y al obtener la varianza explicada se observa que al séptimo componente explica mas del 90% de la relación , para fines de la practica se obtuvieron las proyecciones con 15 componentes
*   Al aplicar FA en el dataframe de numericas se y obtener los eigenvalores se observa que los primeros 8 tienen valores arriba de 1 o cercanos : 3.86620462, 2.03588372, 2.01986692, 1.92271946, 1.81449015, 1.0192355 , 1.01621711, 0.96127742
*  En ambos casos se obtuvo 2 datasets , 1 solo con las variables numéricas y otro con el resto de las variables categóricas






"""